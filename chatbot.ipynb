{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84qDYswDgyfv"
      },
      "source": [
        "Chatbot Development Using Sequence-to-Sequence Model\n",
        "\n",
        "In this guide, we will develop a chatbot using the Sequence-to-Sequence (Seq2Seq) model,\n",
        "leveraging the Cornell Movie Dialogs Corpus.\n",
        "The objective is to create a chatbot that can understand and respond to human queries by learning from movie dialogues."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4MPZCl75o74",
        "outputId": "17026949-7b64-46cc-94a9-ee3b09cdb2f3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTw_sOQ7gyfx"
      },
      "source": [
        "1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqDRB_qJgyfx",
        "outputId": "bb6279ed-ff24-429c-f871-632f8e1274ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.11.0\n",
            "2.11.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install tensorflow==2.11 keras==2.11.0 numpy pandas\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqTkk1-Ogyfy"
      },
      "source": [
        "2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7R2JrGSKgyfy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUGkby7Egyfy"
      },
      "source": [
        "3. Load the Dataset and Convert the JSON data to a pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vK08Hiedgyfy"
      },
      "outputs": [],
      "source": [
        "with open('archive/movie-dialog-corpus-metadata.json', 'r') as file:\n",
        "    data = json.load(file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig2B1jeUgyfy"
      },
      "source": [
        "4. Preprocess the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xDTSp7SOgyfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5eee09e-923f-442c-822e-8036130c5761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 15244\n",
            "Max index in target_sequences: 15243\n"
          ]
        }
      ],
      "source": [
        "# Extract the relevant columns from the 'field' entries\n",
        "columns = [field['source']['extract']['column'] for field in data['recordSet'][0]['field']]\n",
        "\n",
        "# Load the TSV file\n",
        "tsv_path = 'archive/movie_characters_metadata.tsv'\n",
        "df = pd.read_csv(tsv_path, sep='\\t', usecols=columns)\n",
        "\n",
        "# Combine the columns into dialogues (if needed)\n",
        "# Here, we assume each column represents a line of dialogue\n",
        "dialogues = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
        "\n",
        "# Convert to DataFrame\n",
        "dialogues_df = pd.DataFrame(dialogues, columns=['text'])\n",
        "\n",
        "# Extracting dialogues\n",
        "dialogues = dialogues_df['text'].apply(lambda x: x.lower())\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(dialogues)\n",
        "tokenized_dialogues = tokenizer.texts_to_sequences(dialogues)\n",
        "\n",
        "# Padding sequences\n",
        "max_sequence_len = max([len(x) for x in tokenized_dialogues])\n",
        "input_sequences = pad_sequences(tokenized_dialogues, maxlen=max_sequence_len, padding='post')\n",
        "\n",
        "# Add special tokens to the tokenizer after fitting\n",
        "start_token = 'startseq'\n",
        "end_token = 'endseq'\n",
        "tokenizer.word_index[start_token] = len(tokenizer.word_index) + 1\n",
        "tokenizer.word_index[end_token] = len(tokenizer.word_index) + 1\n",
        "tokenizer.index_word[len(tokenizer.word_index)] = start_token\n",
        "tokenizer.index_word[len(tokenizer.word_index) + 1] = end_token\n",
        "\n",
        "# Create target sequences by shifting input sequences by one position\n",
        "target_sequences = np.zeros_like(input_sequences)\n",
        "target_sequences[:, :-1] = input_sequences[:, 1:]\n",
        "target_sequences[:, -1] = tokenizer.word_index[end_token]\n",
        "\n",
        "# Check for any out-of-bound indices\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "print(\"Max index in target_sequences:\", np.max(target_sequences))\n",
        "if np.any(target_sequences >= vocab_size):\n",
        "    print(\"Out of bound indices found\")\n",
        "    out_of_bounds_indices = np.where(target_sequences >= vocab_size)\n",
        "    print(\"Out of bound indices:\", out_of_bounds_indices)\n",
        "    print(\"Target sequences at out of bound indices:\", target_sequences[out_of_bounds_indices])\n",
        "    raise ValueError(\"Some indices in target_sequences are out of bounds.\")\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtQ-MMFvgyfz"
      },
      "source": [
        "5. Create the Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nALqyMrZgyfz"
      },
      "outputs": [],
      "source": [
        "# Define the Seq2Seq model\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space\n",
        "num_tokens = vocab_size\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(num_tokens, latent_dim, mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_tokens, latent_dim, mask_zero=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1ssMdj8gyfz"
      },
      "source": [
        "6. Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tST6mmw-gyfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b50ee4b-0d5d-4107-84af-fe27e6a7a30f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "113/113 [==============================] - 220s 2s/step - loss: 7.2061 - accuracy: 0.1627 - val_loss: 7.5547 - val_accuracy: 0.1860\n",
            "Epoch 2/10\n",
            "113/113 [==============================] - 191s 2s/step - loss: 6.0559 - accuracy: 0.2019 - val_loss: 7.5882 - val_accuracy: 0.2297\n",
            "Epoch 3/10\n",
            "113/113 [==============================] - 210s 2s/step - loss: 5.8106 - accuracy: 0.2163 - val_loss: 7.6365 - val_accuracy: 0.2305\n",
            "Epoch 4/10\n",
            "113/113 [==============================] - 193s 2s/step - loss: 5.6080 - accuracy: 0.2230 - val_loss: 7.5854 - val_accuracy: 0.2368\n",
            "Epoch 5/10\n",
            "113/113 [==============================] - 208s 2s/step - loss: 5.3866 - accuracy: 0.2303 - val_loss: 7.4622 - val_accuracy: 0.2448\n",
            "Epoch 6/10\n",
            "113/113 [==============================] - 190s 2s/step - loss: 5.1835 - accuracy: 0.2336 - val_loss: 7.3900 - val_accuracy: 0.2495\n",
            "Epoch 7/10\n",
            "113/113 [==============================] - 188s 2s/step - loss: 5.0004 - accuracy: 0.2398 - val_loss: 7.3328 - val_accuracy: 0.2569\n",
            "Epoch 8/10\n",
            "113/113 [==============================] - 188s 2s/step - loss: 4.8286 - accuracy: 0.2513 - val_loss: 7.2801 - val_accuracy: 0.2648\n",
            "Epoch 9/10\n",
            "113/113 [==============================] - 208s 2s/step - loss: 4.6642 - accuracy: 0.2738 - val_loss: 7.2215 - val_accuracy: 0.2861\n",
            "Epoch 10/10\n",
            "113/113 [==============================] - 208s 2s/step - loss: 4.5009 - accuracy: 0.2942 - val_loss: 7.1671 - val_accuracy: 0.2975\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x784992502d40>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Assuming you have target_sequences ready\n",
        "model.fit([input_sequences, target_sequences], target_sequences, batch_size=64, epochs=10, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEDPeMS6gyfz"
      },
      "source": [
        "7. Set up the inference models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UvpV8iCggyfz"
      },
      "outputs": [],
      "source": [
        "# Encoder inference model\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x900zCsugyfz"
      },
      "source": [
        "8. Generate Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PERzTAp1gyfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb91b3cf-202a-42f9-921c-771de3b8d0ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Hi there!\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Response:  startseq startseq startseq startseq startseq startseq\n",
            "\n",
            "Input: How are you?\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Response:  man startseq startseq startseq startseq startseq startseq\n",
            "\n",
            "Input: Tell me a joke.\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Response:  man startseq startseq startseq startseq startseq startseq\n",
            "\n",
            "Input: What is your name?\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Response:  man m89 startseq startseq startseq startseq startseq\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Function to preprocess input text\n",
        "def preprocess_input(text):\n",
        "    tokenized_input = tokenizer.texts_to_sequences([text.lower()])\n",
        "    padded_input = pad_sequences(tokenized_input, maxlen=max_sequence_len, padding='post')\n",
        "    return padded_input\n",
        "\n",
        "# Function to generate chatbot responses\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1\n",
        "    target_seq = np.zeros((1, 1))\n",
        "\n",
        "    # Populate the first character of target sequence with the start character\n",
        "    target_seq[0, 0] = tokenizer.word_index['startseq']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = tokenizer.index_word[sampled_token_index]\n",
        "        decoded_sentence += ' ' + sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length or find stop character\n",
        "        if (sampled_char == 'endseq' or len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "# Function to generate response from input text\n",
        "def generate_response(input_text):\n",
        "    input_seq = preprocess_input(input_text)\n",
        "    response = decode_sequence(input_seq)\n",
        "    return response\n",
        "\n",
        "# Test the chatbot\n",
        "test_inputs = [\n",
        "    \"Hi there!\",\n",
        "    \"How are you?\",\n",
        "    \"Tell me a joke.\",\n",
        "    \"What is your name?\"\n",
        "]\n",
        "\n",
        "for input_text in test_inputs:\n",
        "    print(\"Input:\", input_text)\n",
        "    response = generate_response(input_text)\n",
        "    print(\"Response:\", response)\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}